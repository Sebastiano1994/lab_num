{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">  Blocking data and Monte Carlo methods</span>\n",
    "**Es1: Monte Carlo Integration**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first lesson, we've been introduced how to manage data blocking and some first examples of Monte Carlo methods associated.\n",
    "Our first exercise consists in computing the simple, following integral, whose value is aknowledged:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\left< r \\right> = \\int^1_0 r \\, dr = \\frac{1}{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll see soon why integral can be represented as an average in the uniform interval $[0;1]$; as we have a measure, there'll be a specific error associated to this measure. We can do this by use of the central limit theorem: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\int_a^b f(x) \\, dx = (b-a) \\lim_{N \\to \\infty} \\frac{1}{N}\\sum_{i=0}^N f(x_i) = (b-a) \\left<f\\right>_{[a;b]}        $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our interval lies in [0;1], the previous integral can be rewritten as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\int_0^1 r \\, dr = \\left< r \\right>_{[0;1]} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So all that we need is to generate a huge number (\"close to\" $\\infty$) of random variables in a uniform interval [0;1], let's say $10^5$ measures. As infinity is far beyond our computational power, there'll be always an error associated to our measure, a sort of intrinsic bias for our calculation, which must be reported. If the expected value falls in this bias range, everything it's all right, because there'll be always an unavoidable uncertainty linked to every measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nevertheless, there are many methods to lower uncertainty: one of these is data blocking. What we have to do is to split our set of M measures in N subsets: after that, we estimate an average and a variance for each set. At the end we may want to extract a global average and a standard deviation: while the average over averages is nothing but the total average, it's demonstrated that global standard deviation $\\sigma$ is less or equal than standard deviation evaluated without blocking. This can be computed as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sigma = \\sqrt{\\sum_{i=0}^{N} \\frac{\\left<r_i^2\\right>}{N} - \\left(\\frac{\\left<r_i\\right>}{N}\\right)^2  }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, N is the number of blocks, $\\left<r_i\\right>$ and $\\left<r^2_i\\right>$ refer not the single measures, but to the average over a $L=M/N$ number of samples ($L$ is the block lenght). We report the graph of these blocks averages and uncertainties by varying the number of blocks $N$: the lower $N$, the higher the uncertainties (for $N=1$ we have no blocking). Furthermore, the higher the value of $N$, the higher the numbers of data we have to plot. As the previous integral result is well known (call it $r_k=0.5$) and even its uncertainty ($\\sigma_k=1/12$), we'll show how ($\\left<r\\right>-r_k$) value and its uncertainty ($\\left<\\sigma\\right> -\\sigma_k$) will approach null value. Graphs and data are generated by the script.py program:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we go with N=10:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"graphs/av10blocks.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"graphs/var10blocks.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens with N=100:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"graphs/av100blocks.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"graphs/var100blocks.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last, N=1000:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"graphs/av1000blocks.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"graphs/var1000blocks.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last, we may use a $\\chi$ squared test for each of our trials (chisq.py file): we generated $10^6$ random and uniform values, then we divided our interval in 100 bins and the set of measures in $10^4$. Averaging over all $10^4$ sets, we achieved an average of 99.9916, which makes compatible our data distribution with an uniform one. Here follows a graph showing $\\chi_j$ values:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"es1/Chi_squared.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, values are spread around 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Es2: Sampling Distributions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second exercise we want to sample a simple, uniform dice distribution, an \"exponential\" dice and a lorentzian one. This means, we have to generate a random number uniformly in the interval $[0;6]$ for the first case, which is quite simple if you have a good generator; for the latter cases, the work it makes harder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sample a non-uniform distribution we can make use of the cumulative distribution inversion: given a $p(x)$ distribution, its cumulative $F(x)$ is defined as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ F(x) = \\int_{-\\infty}^x p(y) \\, dy $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inversion theorem guarantees us that "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ x = F^{-1}(y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $y$ is a random variable extracted uniformly from $[0;1]$ interval, while $x$ is our $p$-extracted random uniform. All we need to do is to evaluate $F^{-1}(x)$ for $x \\in [0;1]$. For exponential distribution, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ p(x) = e^{-\\lambda x} \\Rightarrow F^{-1}(x) = - \\frac{1}{\\lambda}\\log(1-x) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while for lorentzian distribution we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ p(x) = \\frac{1}{\\pi} \\frac{\\Gamma}{(x-\\mu)^2 + \\Gamma^2} \\Rightarrow F^{-1}(x) = \\frac{1}{\\pi}\\arctan\\left(\\frac{x-\\mu}{\\Gamma}\\right) + \\frac{1}{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our purposes, we'll set $\\lambda, \\Gamma=1$ and $\\mu=0$. Let's generate $10^4$ data for each of these distributions and plot them filling a histogram: data are generated by main.cpp program through an algorithm implemented by random.cpp, then data are exported in linear_1.csv, exp_1.csv and lorentz_1.csv and plotted by histo.py script; before launching, give an additional argument from terminal which is the number of throws $n$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"graphs/dice1.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"graphs/exp1.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"graphs/lorentz1.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we may see how data fit a uniform distribution (dice), an exponential and a lorentzian one. What if we average over data many times? What we have to do is to generate n sets of our $10^4$ data, then compute an average over them and fill a new histogram. Let's try with n=2: this time, data are exported in linear_2.csv, exp_2.csv and lorentz_2.csv, what we obtained is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"graphs/dice2.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"graphs/exp2.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"graphs/lorentz2.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what we observe is that uniform distribution is approaching a gaussian one and the exponential differs a little from the previous one, while the lorentzian seems to be just the same. Let's proceed with n=10 and n=100:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"graphs/dice10.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"graphs/exp10.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"graphs/lorentz10.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"graphs/dice100.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"graphs/exp100.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"graphs/lorentz100.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, all of our distributions approach the gaussian one, except for the lorentzian. This because gaussian and lorentzian distributions are basins of attraction for other distributions: summing up $S_n$ values sampled from an exponential or uniform distribution, these will converge to a gaussian one, while lorentzian ones will converge nothing but to the same lorentzian distribution (as the sum of gaussian values will converge to a gaussian distribution)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Es3: Buffon Experiment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this exercise, we want to riproduce Buffon experiment: we have to simulate many throws of a needle on a grid, then estabilish if the needle crossed grid lines or not. From this, we may give an estimation of $\\pi$ by the following law:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\pi = \\lim_{N_{thr} \\to \\infty}\\frac{2LN_{thr}}{N_{hit}d}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simulate $N \\to \\infty$, let's take a huge value of throws ($10^6$ will be enough), but first of all we need to define a grid, a lenght $L$ between each line and a lenght $d$ for our needle: we set $d$=0.5 and $L$=1, then we generated a five line grid. If the needle crosses one of these lines, i.e. $y_1$ and $y_2$ extremes have different floor ($y_1$=3.94 and $y_2$=4.3 for instance), we update a variable count. Thus, what we have to do is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. generate $y_1$ between 0 and 5 (as a random uniform variable).\n",
    "2. generate $y_2= y_1 + d*cos(\\theta)$, where $\\theta$ is a random angle between $0$ and $2\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we cannot use $\\pi$ to calculate $\\pi$! This means $\\theta$ cannot be $r*2\\pi$, $r \\in [0;1]$, we must search an alternative method. This may be given by the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. generate two random variables x and y between [-1;1]\n",
    "2. if $x^2+y^2<1$, we an define $\\theta$ as $$\\theta = \\cos^{-1}\\left(\\frac{x}{\\sqrt{x^2+y^2}}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take notice, $x$ and $y$ don't matter at all with $y_1$ and $y_2$. Having computed $\\theta$, we may evaluate $y_2$ as suggested before, then we export our data in dati.csv file and we can plot our trend by script.py executable:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"graphs/pi.png\" />\n",
    "The red line shows the true value of $\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
