{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">  Supervised Learning Prediction</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature of these exercise consists in machine learning techniques for prediction of function behaviour: from a set of sampled points, neural networks should be able to rebuild general trend for a generic $f(x)$ function. Furthermore, to simulate real experimental data, we'll try to add some noise on our data: generate $x_i$ data, then our output will be $y_i = f(x_i) + \\eta_i$, with $\\eta_i$ a gaussian random variable such that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $\\left< \\eta_i \\right> = 0 $\n",
    "2. $\\left< \\eta_i \\eta_j \\right> = \\delta_{ij} \\sigma $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By aid of neural networks, our M.L. algorithm should be able to map correctly every Lebesgue-integrable function (Cybenko, 1989, Universal approximation theorem). So let's start from a very simple function, a linear one such as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ f(x) = 2x + 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now study how our predictions may differ fine-tuning hyperparameters such as $\\sigma$, training dataset $N_{train}$ and number of epochs $N_{ep}$, i.e. the number of times our algorithm will make prediction, for a batch of data, to confront with real values; to evaluate the gap between predictions and observations, we've made use of MSE method. Batch size (i.e. the number of data to confront every predicition we make) have been set to 32."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start from $\\sigma=0$: with $N_{train}$=100, we've achieved, for $N_{ep}$=50:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"es1/sigma_0/100_train/50_epochs/Model_loss.png\" />\n",
    "<img src=\"es1/sigma_0/100_train/50_epochs/Predicted_vs_True.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, our prediction is not so far accurate, and both Train and Test loss have to be further minimized. Let's try with $N_{ep}$=100:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"es1/sigma_0/100_train/100_epochs/Model_loss.png\" />\n",
    "<img src=\"es1/sigma_0/100_train/100_epochs/Predicted_vs_True.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better. Slope is almost the same, even though slightly flatter, while loss is asymptotically zero. Can we do better? Let's try with $N_{ep}$=200:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"es1/sigma_0/100_train/200_epochs/Model_loss.png\" />\n",
    "<img src=\"es1/sigma_0/100_train/200_epochs/Predicted_vs_True.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we go: our model has learnt how to fit correctly. It seems to be, the more the epochs, the better the prediction; is it always true? Let's have a check: try with $N_{trial}$=1000 and $N_{ep}$=30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"es1/sigma_0/1000_train/30_epochs/Model_loss.png\" />\n",
    "<img src=\"es1/sigma_0/1000_train/30_epochs/Predicted_vs_True.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the previous parameters have shown good results, these new plots show us the following rule: huge size of data training is the strongest condition for fitting with M.L. techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now go on with $\\sigma$=1: we'll start now from $N_{train}$=1000, with $N_{ep}$=50 and 200:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"es1/sigma_1/1000_training/50_epochs/Model_loss.png\" />\n",
    "<img src=\"es1/sigma_1/1000_training/50_epochs/Predicted_vs_True.png\" />\n",
    "for $N_{ep}$=50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"es1/sigma_1/1000_training/50_epochs/Model_loss.png\" />\n",
    "<img src=\"es1/sigma_1/1000_training/50_epochs/Predicted_vs_True.png\" />\n",
    "for $N_{ep}$=200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows us there's an unavoidable bias between prediction and real model, when introducing some noise on data; raising the number of epochs doesn't seem to be a solution to this gap. Let's try then to increase, another time, the number of training points to $N_{train}$=10000:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"es1/sigma_1/10000_training/50_epochs/Model_loss.png\" />\n",
    "<img src=\"es1/sigma_1/10000_training/50_epochs/Predicted_vs_True.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, there's always a bias, but from here we can reproduce better results: the higher the number of points, the more the fluctuations compensate each other tending to zero (central limit theorem), that's why we have a lower loss on test data rather than on training ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Cubic Function Fit**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make things harder: this time, we'll try to infer a cubic function behaviour, given some (more or less noisy) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
