{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">  Convolutional Neural Networks and Image Recognition </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As last, we'll train our N.N. for image recognition: inputs are given by height $H$ and width $W$ of our picture (i.e. the pixel grid) and the number of channels $D$, which codifies for example image colours. As Convolutional Neural Networks (CNNs) gained a wide success for image recognition, due to their skills of generalization and transitional invariance properties, we will compare these model to the simpler feed-forward one. In this exercize, we'll deal with digit categorization: by MNIST dataset auxilium, we'll try to train our neural networks to classify each of ten digit numbers. Each handwritten image is split in a 28x28 pixel grid, thus our input will be given by 256 neurons (which is nothing but $28^2$). \n",
    "But first, let's try some optimizers: we'll see how accuracy and loss differ when using SDG, Adadelta and Adam for $N_{ep}$=8:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"es1/Loss_sgd.png\" />\n",
    "<img src=\"es1/Accuracy_sgd.png\" />\n",
    "for Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"es1/Loss_adadelta.png\" />\n",
    "<img src=\"es1/Accuracy_adadelta.png\" />\n",
    "for Adadelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"es1/Loss_adam.png\" />\n",
    "<img src=\"es1/Accuracy_adam.png\" />\n",
    "for Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, Adam and Adadelta optimizers perform quite similar: even though they achieve more accurary and less loss rather than SDG, they seem to be more unstable, with training predictions performing even better than the test ones. Stochastic Gradient Descent, however, seems to keep performing better with a higher number of epochs: what if protracting $N_{ep}$ to 16? Let's compare SGD results to Adadelta:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"es1/Loss_sgd2.png\" />\n",
    "<img src=\"es1/Accuracy_sgd2.png\" />\n",
    "for Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"es1/Loss_adadelta2.png\" />\n",
    "<img src=\"es1/Accuracy_adadelta2.png\" />\n",
    "for Adadelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end, Adadelta seems to perform a little bit better, but with unstable outcomes, while SGD prove to be a robust optimization method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convolutional Neural Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we tried to implement CNNs, the scheme we've used is the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"es2/model_plot.png\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the results (with sgd optimizer) for loss and accuracy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"es2/loss.png\" />\n",
    "<img src=\"es2/Accuracy.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy shows to be the best result, considering feed-forward NN too. Here a prediction of some digits:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"es2/prediction.png\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gimp Images recognition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Then we tried to use our trained model to predict some self-made digits (by usage of Gimp application): here we plot some outcomes for 3 epochs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"es3/3EP/0.png\" />\n",
    "<img src=\"es3/3EP/1.png\" />\n",
    "<img src=\"es3/3EP/2.png\" />\n",
    "<img src=\"es3/3EP/3.png\" />\n",
    "<img src=\"es3/3EP/4.png\" />\n",
    "<img src=\"es3/3EP/5.png\" />\n",
    "<img src=\"es3/3EP/6.png\" />\n",
    "<img src=\"es3/3EP/7.png\" />\n",
    "<img src=\"es3/3EP/8.png\" />\n",
    "<img src=\"es3/3EP/9.png\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, some of them are correct, while some others are too grainy for a prediction. With further training, result don't seem to get better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
